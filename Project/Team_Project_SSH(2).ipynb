{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading, basic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "\n",
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"C:/Users/po020/Desktop/KAIST/Python Codes/03. 학과공부/004. 빅데이터분석/02. 팀프로젝트/optiver-realized-volatility-prediction-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 수집\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, 'optiver_data', 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 함수\n",
    "    \n",
    "def load_stock_data(stock_id, directory):\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver_data', directory, f'stock_id={stock_id}'))\n",
    "    \n",
    "def load_data(stock_id, stem, block):\n",
    "    if block == 'train':\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == 'test':\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, 'train'),\n",
    "            load_data(stock_id, stem, 'test')\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id, block='train'):\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "def load_trade(stock_id, block='train'):\n",
    "    return load_data(stock_id, 'trade', block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame({'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4], 'C': np.random.randn(4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th colspan=\"2\" halign=\"left\">B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.097271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.740038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B                   C\n",
       "     sum       std       sum\n",
       "0  1   3  0.707107 -0.097271\n",
       "1  2   7  0.707107 -0.740038"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupby('A').agg({'B':[np.sum, np.std], 'C': np.sum}).reset_index(drop=False)\n",
    "temp.columns = flatten_name('book', temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.188552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.130229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.335772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.843275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B         C\n",
       "0  1  1  1.188552\n",
       "1  1  2 -0.130229\n",
       "2  2  3 -0.335772\n",
       "3  2  4  0.843275"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70911ce1",
   "metadata": {
    "papermill": {
     "duration": 0.021744,
     "end_time": "2021-09-23T13:01:31.835227",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.813483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d50df53",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:31.919509Z",
     "iopub.status.busy": "2021-09-23T13:01:31.908376Z",
     "iopub.status.idle": "2021-09-23T13:01:42.243580Z",
     "shell.execute_reply": "2021-09-23T13:01:42.243112Z",
     "shell.execute_reply.started": "2021-09-22T12:58:10.333189Z"
    },
    "papermill": {
     "duration": 10.386673,
     "end_time": "2021-09-23T13:01:42.243748",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.857075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "# prefix: book or trade\n",
    "# src_names : feature 이름(columns 삽입 - agg.columns)\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c))) # column 이름.book_\n",
    "    return ret\n",
    "\n",
    "\n",
    "def make_book_feature(stock_id, block = 'train'):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    # Calculate spread\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "    \n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "\n",
    "# time별로 묶어서 feature 더 생성\n",
    "    \n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_trade_feature(stock_id, block = 'train'):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "# v2는 price가 들어감\n",
    "# tick size를 계산해서 나오는 열 추가해줌.\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = 'train'):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        price_list = prices.loc[tid].values.flatten()\n",
    "        price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "        ticks[tid] = price_diff[0]\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    #book['tick_size'] = book['time_id'].map(ticks)\n",
    "    \n",
    "    # https://www.kaggle.com/lucasmorin/volatility-maximum-likelihood-estimation/comments#1495387\n",
    "    #book['log_wap'] = np.log(calc_wap1(book))\n",
    "    #book['log_return'] = book.groupby('time_id')['log_wap'].diff()\n",
    "    #book['dt'] = book.groupby('time_id')['seconds_in_bucket'].diff()\n",
    "    #book['inv_dt'] = 1 / book['dt']\n",
    "    #book['estimated_volatility'] = np.power(book['log_return'] / np.sqrt(book['dt']), 2)\n",
    "    #agg = book.groupby('time_id')['estimated_volatility'].sum()\n",
    "    #dst['book.estimated_volatility'] = dst['time_id'].map(agg)\n",
    "    #dst['book.estimated_volatility'] = np.sqrt(dst['book.estimated_volatility'])\n",
    "\n",
    "    #book['log_return_df2'] = book.groupby(['time_id'])['wap1'].apply(log_return_df2)\n",
    "    #book['price_spread_tick'] = (book['ask_price1'] - book['bid_price1']) / book['tick_size']\n",
    "    #agg = book.groupby('time_id')['inv_dt'].mean()\n",
    "    #dst['book.inv_dt.mean'] = dst['time_id'].map(agg)\n",
    "\n",
    "    return dst\n",
    "\n",
    "# trade 데이터의 size랑 order_count\n",
    "# 근데 최종적으로 이사람은 추가 안함.\n",
    "\n",
    "def make_trade_feature_v2(stock_id, block = 'train'):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    features = {\n",
    "        'size':[np.mean],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "# 일단은 make_features 만 하면 괜춘할듯\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "    #with timer('trades(v2)'):\n",
    "    #    trades = Parallel(n_jobs=-1)(delayed(make_trade_feature_v2)(i, block) for i in stock_ids)\n",
    "    #    trade_v2 = pd.concat(trades)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d\n",
    "    #return pd.merge(d, trade_v2, on=['stock_id', 'time_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books]  1106.022sec\n",
      "[trades]  153.998sec\n",
      "[extra features]  2.849sec\n",
      "[books(v2)]  80.117sec\n"
     ]
    }
   ],
   "source": [
    "df = make_features(train, 'train')\n",
    "df = make_features_v2(df, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books]  0.206sec\n",
      "[trades]  0.071sec\n",
      "[extra features]  0.013sec\n",
      "[books(v2)]  0.029sec\n",
      "(428932, 216)\n",
      "(3, 216)\n"
     ]
    }
   ],
   "source": [
    "# test set에 대해서 만들어줌\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, 'optiver_data', 'test.csv'))\n",
    "# if len(test) == 3:\n",
    "#     IS_1ST_STAGE = True\n",
    "\n",
    "test_df = make_features(test, 'test')\n",
    "test_df = make_features_v2(test_df, 'test')\n",
    "\n",
    "print(df.shape)\n",
    "print(test_df.shape)\n",
    "df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)\n",
    "\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "# #df['book.log_return1.realized_volatility.sq'] = np.power(df['book.log_return1.realized_volatility'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[knn fit]  169.697sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "N_NEIGHBORS_MAX = 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, pivot, p, metric='minkowski', metric_params=None):\n",
    "        nn = NearestNeighbors(n_neighbors=N_NEIGHBORS_MAX, p=p, metric=metric, metric_params=metric_params)\n",
    "        nn.fit(pivot)\n",
    "        self.distances, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size']\n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "####################################################################################\n",
    "# price neighbor\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'price') # index: time_id, column: stock_id, value: price\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_p2 = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_p1 = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'VI':np.cov(pivot.values.T)})\n",
    "    k_neighbors_stock = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "####################################################################################\n",
    "# vol neighbor\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "\n",
    "    #pivot = pd.concat([df_pv.pivot('time_id', 'stock_id', 'vol'), df_pv.pivot('time_id', 'stock_id', 'trade.tau')], axis=1).copy()\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_vol = Neighbors(pivot, 1)\n",
    "    k_neighbors_stock_vol = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "####################################################################################\n",
    "# trade.size.num neighbor\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    k_neighbors_size = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'VI':np.cov(pivot.values.T)})\n",
    "    k_neighbors_size_p2 = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_stock_size = Neighbors(minmax_scale(pivot.transpose()), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neighbors(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "    feature_pivot.head()\n",
    "    \n",
    "    neighbors = np.zeros((n, *feature_pivot.shape)) #dynamic instance : * -> 계속해서 변동할때 사용.\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[k_neighbors[:, i], :]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_neighbors_stock(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "    feature_pivot.head()\n",
    "    \n",
    "    neighbors = np.zeros((n, *feature_pivot.shape))\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[:, k_neighbors[:, i]]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_nn_feature(df, neighbors, columns, index, n=5, agg=np.mean, postfix='', exclude_self=False, exact=False):\n",
    "    start = 1 if exclude_self else 0\n",
    "    \n",
    "    if exact:\n",
    "        pivot_aggs = pd.DataFrame(neighbors[n-1,:,:], columns=columns, index=index)\n",
    "    else:\n",
    "        pivot_aggs = pd.DataFrame(agg(neighbors[start:n,:,:], axis=0), columns=columns, index=index)\n",
    "    dst = pivot_aggs.unstack().reset_index() # unstack(level)이 의미하는 것은 multi Index의 몇번째 index를 칼럼 방향으로 stacking 할것인가를 의미한다\n",
    "    dst.columns = ['stock_id', 'time_id', f'{feature_col}_cluster{n}{postfix}_{agg.__name__}']\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           1\n",
       "2           5\n",
       "3           6\n",
       "4           7\n",
       "         ... \n",
       "917548    568\n",
       "917549    569\n",
       "917550    571\n",
       "917551    572\n",
       "917552    582\n",
       "Name: seconds_in_bucket, Length: 917553, dtype: int16"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['seconds_in_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame()\n",
    "temp2[temp.columns[-1]]= temp[temp.columns[-1]].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.817565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.049585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          C\n",
       "0  0.720294\n",
       "1 -0.817565\n",
       "2  0.309547\n",
       "3 -1.049585"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.817565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.309547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.049585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B         C\n",
       "0  1  1  0.720294\n",
       "1  1  2 -0.817565\n",
       "2  2  3  0.309547\n",
       "3  2  4 -1.049585"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 219)\n",
      "(428935, 249)\n"
     ]
    }
   ],
   "source": [
    "import gc #순환참조를 탐지하고 해결하기 위해 사용하는 모듈\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "def rank_data(a, axis=None):\n",
    "    return a[0] - np.min(a, axis=axis)\n",
    "# 왜만든거지?\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df2 = df.copy()\n",
    "print(df2.shape)\n",
    "\n",
    "df2['real_price'] = 0.01 / df2['tick_size']\n",
    "del df2['tick_size']\n",
    "\n",
    "# relative rank\n",
    "df2['trade.order_count.mean'] = df2.groupby(\n",
    "    'time_id')['trade.order_count.mean'].rank()\n",
    "df2['book.total_volume.sum'] = df2.groupby(\n",
    "    'time_id')['book.total_volume.sum'].rank()\n",
    "df2['book.total_volume.mean'] = df2.groupby(\n",
    "    'time_id')['book.total_volume.mean'].rank()\n",
    "df2['book.total_volume.std'] = df2.groupby(\n",
    "    'time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "# time-id 별 trade.order_count와 book의 total_volumne에 대한 sum, mean, std값의 순위를 매긴 컬럼 추가\n",
    "\n",
    "df2['trade.tau'] = df2.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "#df2['trade_300.tau'] = df2.groupby('time_id')['trade_300.tau'].rank()\n",
    "#df2['book.total_volume.std'] = df2.groupby('time_id')['book.total_volume.std'].rank()\n",
    "#df2['trade.size.sum'] = df2.groupby('time_id')['trade.size.sum'].rank()\n",
    "\n",
    "for dt in [150, 300, 450]:\n",
    "    df2[f'book_{dt}.total_volume.sum'] = df2.groupby(\n",
    "        'time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "    df2[f'book_{dt}.total_volume.mean'] = df2.groupby(\n",
    "        'time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "    df2[f'book_{dt}.total_volume.std'] = df2.groupby(\n",
    "        'time_id')[f'book_{dt}.total_volume.std'].rank()\n",
    "    df2[f'trade_{dt}.order_count.mean'] = df2.groupby(\n",
    "        'time_id')[f'trade_{dt}.order_count.mean'].rank()\n",
    "\n",
    "# df2.groupby('time_id')['book.volume_imbalance.sum'].rank()\n",
    "#df2['tick_size'] = df2.groupby('time_id')['tick_size'].rank()\n",
    "\n",
    "## neighbor stock id 에 대한 feature\n",
    "feature_cols_stock = {\n",
    "    'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "\n",
    "    'trade.seconds_in_bucket.count': [np.mean],\n",
    "    'trade.tau': [np.mean],\n",
    "    'trade_150.tau': [np.mean],\n",
    "    'book.tau': [np.mean],\n",
    "    'trade.size.sum': [np.mean],\n",
    "    'book.seconds_in_bucket.count': [np.mean],\n",
    "\n",
    "    # 'trade.order_count.mean': [np.mean],\n",
    "    # 'avg_time_vol': [np.mean],\n",
    "    # 'trade_150.tau': [np.mean],\n",
    "    # 'trade_450.tau': [np.mean],\n",
    "    # 'book.total_volume.sum': [np.mean],\n",
    "    # 'book.volume_imbalance.mean': [np.mean],\n",
    "}\n",
    "\n",
    "\n",
    "## neighbor time id 에 대한 feature\n",
    "feature_cols = {\n",
    "    'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "    # 'book_150.log_return1.realized_volatility': [np.mean, np.min],\n",
    "    # 'book_300.log_return1.realized_volatility': [np.mean, np.min],\n",
    "    # 'book_450.log_return1.realized_volatility': [np.mean, np.min],\n",
    "\n",
    "    'real_price': [np.max, np.mean, np.min],\n",
    "\n",
    "    'trade.seconds_in_bucket.count': [np.mean],\n",
    "    'trade.tau': [np.mean],\n",
    "    'trade.size.sum': [np.mean],\n",
    "    'book.seconds_in_bucket.count': [np.mean],\n",
    "\n",
    "    # 비슷한 volatility trend를 갖는 20개의 주식종목의 last 300sec에 대한 average tau (average at close time)\n",
    "    'trade_150.tau_cluster20_sv_mean': [np.mean],\n",
    "    'trade.size.sum_cluster20_sv_mean': [np.mean],\n",
    "    # 'book.log_return1.realized_volatility.sq': [np.sum],\n",
    "    # 'book.estimated_volatility': [np.mean],\n",
    "    # 'trade.order_count.mean': [np.mean],\n",
    "    # 'book.total_volume.sum': [np.mean],\n",
    "    # 'book.volume_imbalance.mean': [np.mean],\n",
    "    # 'avg_time_vol': [np.mean],\n",
    "    # 'book.ask_spread.mean': [np.mean],\n",
    "    # 'book.bid_spread.mean': [np.mean],\n",
    "}\n",
    "\n",
    "time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "ndf = None\n",
    "\n",
    "cols = []\n",
    "\n",
    "\n",
    "# ndf가 아무것도 없으면 dst 그대로 반환\n",
    "# ndf가 있으면 dst의 마지막 열을 ndf에 추가해서 ndf 반환\n",
    "def _add_ndf(ndf, dst):\n",
    "    if ndf is None:\n",
    "        return dst\n",
    "    else:\n",
    "        ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "        return ndf\n",
    "\n",
    "\n",
    "# neighbor stock_id\n",
    "for feature_col in feature_cols_stock.keys():\n",
    "    # feature_pivot, neighbors_stock = make_neighbors_stock(\n",
    "    #     df2, k_neighbors_stock.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    feature_pivot, neighbors_stock_vol = make_neighbors_stock(\n",
    "        df2, k_neighbors_stock_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_stock_size = make_neighbors_stock(\n",
    "        df2, k_neighbors_stock_size.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "\n",
    "    columns = feature_pivot.columns\n",
    "    index = feature_pivot.index\n",
    "\n",
    "    for agg in feature_cols_stock[feature_col]:\n",
    "        for n in stock_id_neighbor_sizes:\n",
    "            exclude_self = True\n",
    "            exact = False\n",
    "            # dst = make_nn_feature(df2, neighbors_stock, columns, index, n=n, agg=agg, postfix='_s',\n",
    "            #                       exclude_self=exclude_self, exact=exact)\n",
    "            # ndf = _add_ndf(ndf, dst)\n",
    "            dst = make_nn_feature(df2, neighbors_stock_vol, columns, index, n=n, agg=agg, postfix='_sv',\n",
    "                                  exclude_self=exclude_self, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            # dst = make_nn_feature(df2, neighbors_stock_size, columns, index, n=n, agg=agg, postfix='_ssize',\n",
    "            #                     exclude_self=exclude_self)\n",
    "            #ndf = _add_ndf(ndf, dst)\n",
    "    del feature_pivot, neighbors_stock_vol\n",
    "\n",
    "df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "ndf = None\n",
    "\n",
    "# neighbor time_id\n",
    "# ###############################################################################\n",
    "# for feature_col in feature_cols.keys():\n",
    "#     feature_pivot, neighbors = make_neighbors(\n",
    "#         df2, k_neighbors_p2.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "#     _, neighbors_p1 = make_neighbors(\n",
    "#         df2, k_neighbors_p1.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "#     _, neighbors_vol = make_neighbors(\n",
    "#         df2, k_neighbors_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "#     _, neighbors_size = make_neighbors(\n",
    "#         df2, k_neighbors_size.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "#     _, neighbors_size_p2 = make_neighbors(\n",
    "#         df2, k_neighbors_size_p2.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "\n",
    "#     columns = feature_pivot.columns\n",
    "#     index = feature_pivot.index\n",
    "\n",
    "#     if 'volatility' in feature_col:\n",
    "#         time_id_ns = time_id_neigbor_sizes_vol\n",
    "#     else:\n",
    "#         time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "#     for agg in feature_cols[feature_col]:\n",
    "#         for n in time_id_ns:\n",
    "#             exclude_self = True  # n >= 10\n",
    "#             exclude_self2 = False\n",
    "#             exact = False\n",
    "\n",
    "#             # if n <= 40:\n",
    "#             dst = make_nn_feature(df2, neighbors, columns, index, n=n, agg=agg, postfix='_p2',\n",
    "#                                   exclude_self=exclude_self, exact=exact)\n",
    "#             ndf = _add_ndf(ndf, dst)\n",
    "#             dst = make_nn_feature(df2, neighbors_p1, columns, index, n=n, agg=agg, postfix='_p1',\n",
    "#                                   exclude_self=exclude_self2, exact=exact)\n",
    "#             ndf = _add_ndf(ndf, dst)\n",
    "\n",
    "#             dst = make_nn_feature(df2, neighbors_vol, columns, index, n=n, agg=agg, postfix='_v',\n",
    "#                                   exclude_self=exclude_self2, exact=exact)\n",
    "#             ndf = _add_ndf(ndf, dst)\n",
    "#             dst = make_nn_feature(df2, neighbors_size, columns, index, n=n, agg=agg, postfix='_size',\n",
    "#                                   exclude_self=exclude_self2, exact=exact)\n",
    "#             ndf = _add_ndf(ndf, dst)\n",
    "#             dst = make_nn_feature(df2, neighbors_size_p2, columns, index, n=n, agg=agg, postfix='_size_p2',\n",
    "#                                   exclude_self=exclude_self2, exact=exact)\n",
    "#             ndf = _add_ndf(ndf, dst)\n",
    "#             cols.append(dst.columns[-1])\n",
    "\n",
    "#     del feature_pivot, neighbors, neighbors_p1, neighbors_vol, neighbors_size, neighbors_size_p2\n",
    "\n",
    "# df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "# # 주가는 그 자체를 feature로 사용하는게 아니라 time_id 의 nearest를 측정하기 위해 사용됨.\n",
    "\n",
    "# for sz in time_id_neigbor_sizes:\n",
    "#     df2[f'real_price_rankmin_{sz}'] = df2['real_price'] / \\\n",
    "#         df2[f\"real_price_cluster{sz}_p2_amin\"]\n",
    "#     df2[f'real_price_rankmax_{sz}'] = df2['real_price'] / \\\n",
    "#         df2[f\"real_price_cluster{sz}_p2_amax\"]\n",
    "#     df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / \\\n",
    "#         df2[f\"real_price_cluster{sz}_p2_mean\"]\n",
    "\n",
    "# for sz in time_id_neigbor_sizes_vol:\n",
    "#     df2[f'vol_rankmin_{sz}'] = df2['book.log_return1.realized_volatility'] / \\\n",
    "#         df2[f\"book.log_return1.realized_volatility_cluster{sz}_p2_amin\"]\n",
    "#     df2[f'vol_rankmax_{sz}'] = df2['book.log_return1.realized_volatility'] / \\\n",
    "#         df2[f\"book.log_return1.realized_volatility_cluster{sz}_p2_amax\"]\n",
    "\n",
    "# price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "# for c in price_cols:\n",
    "#     del df2[c]\n",
    "\n",
    "# #df2['book.log_return1.realized_volatility_rank'] = df2.groupby('time_id')['book.log_return1.realized_volatility'].rank()\n",
    "# #df2['book.log_return1.realized_volatility_cluster3_size_mean_rank'] = df2.groupby('time_id')['book.log_return1.realized_volatility_cluster3_size_mean'].rank()\n",
    "\n",
    "# for sz in time_id_neigbor_sizes_vol:\n",
    "#     tgt = f'book.log_return1.realized_volatility_cluster{sz}_p1_mean'\n",
    "#     df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:   \n",
    "    for check in cols_to_log:\n",
    "        if check in c:\n",
    "            df2[c] = np.log(df2[c]+1)\n",
    "            break\n",
    "\n",
    "print(df2.shape)\n",
    "df2.reset_index(drop=True)\n",
    "\n",
    "del ndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428935, 219)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_unq = set(df2.columns) - set(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_unq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book.log_return1.realized_volatility_cluster10_sv_amax',\n",
       " 'book.log_return1.realized_volatility_cluster10_sv_amin',\n",
       " 'book.log_return1.realized_volatility_cluster10_sv_mean',\n",
       " 'book.log_return1.realized_volatility_cluster10_sv_std',\n",
       " 'book.log_return1.realized_volatility_cluster20_sv_amax',\n",
       " 'book.log_return1.realized_volatility_cluster20_sv_amin',\n",
       " 'book.log_return1.realized_volatility_cluster20_sv_mean',\n",
       " 'book.log_return1.realized_volatility_cluster20_sv_std',\n",
       " 'book.log_return1.realized_volatility_cluster40_sv_amax',\n",
       " 'book.log_return1.realized_volatility_cluster40_sv_amin',\n",
       " 'book.log_return1.realized_volatility_cluster40_sv_mean',\n",
       " 'book.log_return1.realized_volatility_cluster40_sv_std',\n",
       " 'book.seconds_in_bucket.count_cluster10_sv_mean',\n",
       " 'book.seconds_in_bucket.count_cluster20_sv_mean',\n",
       " 'book.seconds_in_bucket.count_cluster40_sv_mean',\n",
       " 'book.tau_cluster10_sv_mean',\n",
       " 'book.tau_cluster20_sv_mean',\n",
       " 'book.tau_cluster40_sv_mean',\n",
       " 'real_price',\n",
       " 'trade.seconds_in_bucket.count_cluster10_sv_mean',\n",
       " 'trade.seconds_in_bucket.count_cluster20_sv_mean',\n",
       " 'trade.seconds_in_bucket.count_cluster40_sv_mean',\n",
       " 'trade.size.sum_cluster10_sv_mean',\n",
       " 'trade.size.sum_cluster20_sv_mean',\n",
       " 'trade.size.sum_cluster40_sv_mean',\n",
       " 'trade.tau_cluster10_sv_mean',\n",
       " 'trade.tau_cluster20_sv_mean',\n",
       " 'trade.tau_cluster40_sv_mean',\n",
       " 'trade_150.tau_cluster10_sv_mean',\n",
       " 'trade_150.tau_cluster20_sv_mean',\n",
       " 'trade_150.tau_cluster40_sv_mean'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_unq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 symbol에 대해 비슷한 trading volume을 가진 RV의 평균\n",
    "\n",
    "df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "df2['realized_volatility_roll3_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(3, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values\n",
    "df2['realized_volatility_roll10_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(10, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>book.seconds_in_bucket.count</th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap1.mean</th>\n",
       "      <th>book.wap1.std</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "      <th>book.wap2.mean</th>\n",
       "      <th>book.wap2.std</th>\n",
       "      <th>...</th>\n",
       "      <th>book.tau_cluster20_sv_mean</th>\n",
       "      <th>book.tau_cluster40_sv_mean</th>\n",
       "      <th>trade.size.sum_cluster10_sv_mean</th>\n",
       "      <th>trade.size.sum_cluster20_sv_mean</th>\n",
       "      <th>trade.size.sum_cluster40_sv_mean</th>\n",
       "      <th>book.seconds_in_bucket.count_cluster10_sv_mean</th>\n",
       "      <th>book.seconds_in_bucket.count_cluster20_sv_mean</th>\n",
       "      <th>book.seconds_in_bucket.count_cluster40_sv_mean</th>\n",
       "      <th>realized_volatility_roll3_by_book.total_volume.mean</th>\n",
       "      <th>realized_volatility_roll10_by_book.total_volume.mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1176</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>144.0</td>\n",
       "      <td>143.815068</td>\n",
       "      <td>0.998716</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>143.849316</td>\n",
       "      <td>0.998954</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047374</td>\n",
       "      <td>0.048299</td>\n",
       "      <td>9.806181</td>\n",
       "      <td>10.223156</td>\n",
       "      <td>10.523262</td>\n",
       "      <td>440.666656</td>\n",
       "      <td>465.315796</td>\n",
       "      <td>461.000000</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.003038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8664</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>147.0</td>\n",
       "      <td>146.899894</td>\n",
       "      <td>0.999319</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>146.901871</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056475</td>\n",
       "      <td>0.055844</td>\n",
       "      <td>9.127394</td>\n",
       "      <td>9.754560</td>\n",
       "      <td>10.304124</td>\n",
       "      <td>316.666656</td>\n",
       "      <td>340.210541</td>\n",
       "      <td>347.589752</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.003409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12758</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>142.0</td>\n",
       "      <td>141.728688</td>\n",
       "      <td>0.998089</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>141.709407</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050528</td>\n",
       "      <td>0.052232</td>\n",
       "      <td>9.511703</td>\n",
       "      <td>9.618899</td>\n",
       "      <td>9.832170</td>\n",
       "      <td>383.555542</td>\n",
       "      <td>419.157898</td>\n",
       "      <td>402.846161</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>19033</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.842941</td>\n",
       "      <td>0.998329</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>93.848332</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>0.058893</td>\n",
       "      <td>8.882715</td>\n",
       "      <td>9.349086</td>\n",
       "      <td>9.438289</td>\n",
       "      <td>282.444458</td>\n",
       "      <td>328.315796</td>\n",
       "      <td>325.128204</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20499</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>170.0</td>\n",
       "      <td>169.654033</td>\n",
       "      <td>0.997965</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>169.650958</td>\n",
       "      <td>0.997947</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054286</td>\n",
       "      <td>0.054694</td>\n",
       "      <td>9.711109</td>\n",
       "      <td>9.795882</td>\n",
       "      <td>9.855143</td>\n",
       "      <td>350.555542</td>\n",
       "      <td>370.684204</td>\n",
       "      <td>373.282043</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.002721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>16402</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>307.0</td>\n",
       "      <td>307.085638</td>\n",
       "      <td>1.000279</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>307.023339</td>\n",
       "      <td>1.000076</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052267</td>\n",
       "      <td>0.056501</td>\n",
       "      <td>9.809000</td>\n",
       "      <td>10.152408</td>\n",
       "      <td>10.100743</td>\n",
       "      <td>419.555542</td>\n",
       "      <td>408.842102</td>\n",
       "      <td>358.410248</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.012807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>4927</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.815252</td>\n",
       "      <td>1.003640</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>224.783674</td>\n",
       "      <td>1.003499</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052116</td>\n",
       "      <td>0.054666</td>\n",
       "      <td>10.812129</td>\n",
       "      <td>10.727410</td>\n",
       "      <td>10.774607</td>\n",
       "      <td>397.111115</td>\n",
       "      <td>398.842102</td>\n",
       "      <td>371.538452</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.011674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428932</th>\n",
       "      <td>126</td>\n",
       "      <td>15155</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>348.0</td>\n",
       "      <td>337.124379</td>\n",
       "      <td>0.968748</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>336.886673</td>\n",
       "      <td>0.968065</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047758</td>\n",
       "      <td>0.050098</td>\n",
       "      <td>11.327068</td>\n",
       "      <td>11.372158</td>\n",
       "      <td>11.323144</td>\n",
       "      <td>476.222229</td>\n",
       "      <td>456.526306</td>\n",
       "      <td>428.641022</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.011329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428933</th>\n",
       "      <td>126</td>\n",
       "      <td>13316</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>279.0</td>\n",
       "      <td>281.655248</td>\n",
       "      <td>1.009517</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>281.599159</td>\n",
       "      <td>1.009316</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051509</td>\n",
       "      <td>0.056141</td>\n",
       "      <td>11.187591</td>\n",
       "      <td>10.930043</td>\n",
       "      <td>10.774504</td>\n",
       "      <td>405.444458</td>\n",
       "      <td>414.421051</td>\n",
       "      <td>367.743591</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428934</th>\n",
       "      <td>126</td>\n",
       "      <td>1464</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>506.0</td>\n",
       "      <td>503.463445</td>\n",
       "      <td>0.994987</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>503.465329</td>\n",
       "      <td>0.994991</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044869</td>\n",
       "      <td>0.046808</td>\n",
       "      <td>10.357708</td>\n",
       "      <td>10.635717</td>\n",
       "      <td>10.397366</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>504.105255</td>\n",
       "      <td>470.641022</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.012813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428935 rows × 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target  book.seconds_in_bucket.count  \\\n",
       "0              0     1176  0.005746                         144.0   \n",
       "1              0     8664  0.002469                         147.0   \n",
       "2              0    12758  0.002541                         142.0   \n",
       "3              0    19033  0.002515                          94.0   \n",
       "4              0    20499  0.003066                         170.0   \n",
       "...          ...      ...       ...                           ...   \n",
       "428930       126    16402  0.004163                         307.0   \n",
       "428931       126     4927  0.005008                         224.0   \n",
       "428932       126    15155  0.018900                         348.0   \n",
       "428933       126    13316  0.010262                         279.0   \n",
       "428934       126     1464  0.005431                         506.0   \n",
       "\n",
       "        book.wap1.sum  book.wap1.mean  book.wap1.std  book.wap2.sum  \\\n",
       "0          143.815068        0.998716       0.001774     143.849316   \n",
       "1          146.899894        0.999319       0.000366     146.901871   \n",
       "2          141.728688        0.998089       0.000900     141.709407   \n",
       "3           93.842941        0.998329       0.000771      93.848332   \n",
       "4          169.654033        0.997965       0.000716     169.650958   \n",
       "...               ...             ...            ...            ...   \n",
       "428930     307.085638        1.000279       0.002120     307.023339   \n",
       "428931     224.815252        1.003640       0.000781     224.783674   \n",
       "428932     337.124379        0.968748       0.006521     336.886673   \n",
       "428933     281.655248        1.009517       0.002213     281.599159   \n",
       "428934     503.463445        0.994987       0.000881     503.465329   \n",
       "\n",
       "        book.wap2.mean  book.wap2.std  ...  book.tau_cluster20_sv_mean  \\\n",
       "0             0.998954       0.001861  ...                    0.047374   \n",
       "1             0.999332       0.000398  ...                    0.056475   \n",
       "2             0.997954       0.000960  ...                    0.050528   \n",
       "3             0.998387       0.000798  ...                    0.057823   \n",
       "4             0.997947       0.000761  ...                    0.054286   \n",
       "...                ...            ...  ...                         ...   \n",
       "428930        1.000076       0.002213  ...                    0.052267   \n",
       "428931        1.003499       0.000937  ...                    0.052116   \n",
       "428932        0.968065       0.006172  ...                    0.047758   \n",
       "428933        1.009316       0.002231  ...                    0.051509   \n",
       "428934        0.994991       0.000936  ...                    0.044869   \n",
       "\n",
       "        book.tau_cluster40_sv_mean  trade.size.sum_cluster10_sv_mean  \\\n",
       "0                         0.048299                          9.806181   \n",
       "1                         0.055844                          9.127394   \n",
       "2                         0.052232                          9.511703   \n",
       "3                         0.058893                          8.882715   \n",
       "4                         0.054694                          9.711109   \n",
       "...                            ...                               ...   \n",
       "428930                    0.056501                          9.809000   \n",
       "428931                    0.054666                         10.812129   \n",
       "428932                    0.050098                         11.327068   \n",
       "428933                    0.056141                         11.187591   \n",
       "428934                    0.046808                         10.357708   \n",
       "\n",
       "        trade.size.sum_cluster20_sv_mean  trade.size.sum_cluster40_sv_mean  \\\n",
       "0                              10.223156                         10.523262   \n",
       "1                               9.754560                         10.304124   \n",
       "2                               9.618899                          9.832170   \n",
       "3                               9.349086                          9.438289   \n",
       "4                               9.795882                          9.855143   \n",
       "...                                  ...                               ...   \n",
       "428930                         10.152408                         10.100743   \n",
       "428931                         10.727410                         10.774607   \n",
       "428932                         11.372158                         11.323144   \n",
       "428933                         10.930043                         10.774504   \n",
       "428934                         10.635717                         10.397366   \n",
       "\n",
       "        book.seconds_in_bucket.count_cluster10_sv_mean  \\\n",
       "0                                           440.666656   \n",
       "1                                           316.666656   \n",
       "2                                           383.555542   \n",
       "3                                           282.444458   \n",
       "4                                           350.555542   \n",
       "...                                                ...   \n",
       "428930                                      419.555542   \n",
       "428931                                      397.111115   \n",
       "428932                                      476.222229   \n",
       "428933                                      405.444458   \n",
       "428934                                      508.000000   \n",
       "\n",
       "        book.seconds_in_bucket.count_cluster20_sv_mean  \\\n",
       "0                                           465.315796   \n",
       "1                                           340.210541   \n",
       "2                                           419.157898   \n",
       "3                                           328.315796   \n",
       "4                                           370.684204   \n",
       "...                                                ...   \n",
       "428930                                      408.842102   \n",
       "428931                                      398.842102   \n",
       "428932                                      456.526306   \n",
       "428933                                      414.421051   \n",
       "428934                                      504.105255   \n",
       "\n",
       "        book.seconds_in_bucket.count_cluster40_sv_mean  \\\n",
       "0                                           461.000000   \n",
       "1                                           347.589752   \n",
       "2                                           402.846161   \n",
       "3                                           325.128204   \n",
       "4                                           373.282043   \n",
       "...                                                ...   \n",
       "428930                                      358.410248   \n",
       "428931                                      371.538452   \n",
       "428932                                      428.641022   \n",
       "428933                                      367.743591   \n",
       "428934                                      470.641022   \n",
       "\n",
       "        realized_volatility_roll3_by_book.total_volume.mean  \\\n",
       "0                                                0.004140     \n",
       "1                                                0.003729     \n",
       "2                                                0.002643     \n",
       "3                                                0.002304     \n",
       "4                                                0.003089     \n",
       "...                                                   ...     \n",
       "428930                                           0.010648     \n",
       "428931                                           0.013510     \n",
       "428932                                           0.014948     \n",
       "428933                                           0.014979     \n",
       "428934                                           0.009054     \n",
       "\n",
       "        realized_volatility_roll10_by_book.total_volume.mean  \n",
       "0                                                0.003038     \n",
       "1                                                0.003409     \n",
       "2                                                0.002964     \n",
       "3                                                0.002818     \n",
       "4                                                0.002721     \n",
       "...                                                   ...     \n",
       "428930                                           0.012807     \n",
       "428931                                           0.011674     \n",
       "428932                                           0.011329     \n",
       "428933                                           0.011402     \n",
       "428934                                           0.012813     \n",
       "\n",
       "[428935 rows x 251 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock-id Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자체 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# lda_n = 3\n",
    "# lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "# stock_id_emb = pd.DataFrame(lda.fit_transform(pivot.transpose()), index=df_pv.pivot('time_id', 'stock_id', 'vol').columns)\n",
    "\n",
    "# for i in range(lda_n):\n",
    "#     df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4235"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = df2[~df2.target.isnull()].copy()\n",
    "# df_test = df2[df2.target.isnull()].copy()\n",
    "# del df2\n",
    "# gc.collect()b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ef3c2",
   "metadata": {
    "papermill": {
     "duration": 3.346055,
     "end_time": "2021-09-23T13:09:58.654302",
     "exception": false,
     "start_time": "2021-09-23T13:09:55.308247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4790c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:58.729416Z",
     "iopub.status.busy": "2021-09-23T13:09:58.728781Z",
     "iopub.status.idle": "2021-09-23T13:09:58.793253Z",
     "shell.execute_reply": "2021-09-23T13:09:58.792795Z",
     "shell.execute_reply.started": "2021-09-22T13:08:11.89487Z"
    },
    "papermill": {
     "duration": 0.114538,
     "end_time": "2021-09-23T13:09:58.793362",
     "exception": false,
     "start_time": "2021-09-23T13:09:58.678824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "%matplotlib inline    \n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    # AMZN\n",
    "    plt.plot(dft['stock_id=61'])\n",
    "    plt.plot(dft['stock_id=37'])\n",
    "    plt.plot(dft['stock_id=113'])\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet')}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "        df_target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "        df_target = df_target.groupby('time_id').target.mean()\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b4c87ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:58.847979Z",
     "iopub.status.busy": "2021-09-23T13:09:58.847439Z",
     "iopub.status.idle": "2021-09-23T13:10:00.679256Z",
     "shell.execute_reply": "2021-09-23T13:10:00.679671Z",
     "shell.execute_reply.started": "2021-09-22T13:08:12.008144Z"
    },
    "papermill": {
     "duration": 1.862807,
     "end_time": "2021-09-23T13:10:00.679832",
     "exception": false,
     "start_time": "2021-09-23T13:09:58.817025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeid_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\po020\\Desktop\\KAIST\\Python Codes\\03. 학과공부\\004. 빅데이터분석\\02. 팀프로젝트\\Team_Project_SSH(2).ipynb 셀 35\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# with timer('calculate order of time-id'):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#     if USE_PRECOMPUTE_FEATURES:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#         timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     else:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#         timeid_order = reconstruct_time_id_order()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m timer(\u001b[39m'\u001b[39m\u001b[39mmake folds\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     timeid_order[\u001b[39m'\u001b[39m\u001b[39mtime_id_order\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(timeid_order))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     df_train[\u001b[39m'\u001b[39m\u001b[39mtime_id_order\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train[\u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(timeid_order\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mtime_id_order\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/po020/Desktop/KAIST/Python%20Codes/03.%20%ED%95%99%EA%B3%BC%EA%B3%B5%EB%B6%80/004.%20%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/02.%20%ED%8C%80%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/Team_Project_SSH%282%29.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     df_train \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39msort_values([\u001b[39m'\u001b[39m\u001b[39mtime_id_order\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstock_id\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeid_order' is not defined"
     ]
    }
   ],
   "source": [
    "timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n",
    "\n",
    "with timer('make folds'):\n",
    "    timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "    df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "    df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "    time_id_orders = df_train['time_id_order']\n",
    "\n",
    "    folds = []\n",
    "    for i, border in enumerate(folds_border):\n",
    "        idx_train = np.where(time_id_orders < border)[0]\n",
    "        idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "        folds.append((idx_train, idx_valid))\n",
    "        \n",
    "        print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "        \n",
    "del df_train['time_id_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
