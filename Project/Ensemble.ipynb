{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.exmaples (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow.exmaples\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class model:\n",
    "\n",
    "    def __init__ (self, sess, name):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_numbers = 10\n",
    "# models = list()\n",
    "\n",
    "# for idx in range (n_numbers):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_true-y_pred)/y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('./397/X_train_tf.pkl').astype(float)\n",
    "y_train = pd.read_pickle('./397/y_train_tf.pkl').astype(float)\n",
    "X_val = pd.read_pickle('./397/X_val_tf.pkl').astype(float)\n",
    "y_val = pd.read_pickle('./397/y_val_tf.pkl').astype(float)\n",
    "X_test = pd.read_pickle('./397/X_test_tf.pkl').astype(float)\n",
    "y_test = pd.read_pickle('./397/y_test_tf.pkl').astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(198, 49, 1), activation='relu', solver='adam', alpha=0.0001, batch_size=500, max_iter=100, random_state=777, verbose=True, early_stopping=True, n_iter_no_change=10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=198,\n",
    "        input_dim=X_train.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        activation='ReLU'\n",
    "    ))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=49,\n",
    "        input_dim=198,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        activation='ReLU'\n",
    "    ))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        input_dim=49,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "))\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=rmspe)\n",
    "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_delta=1e-5, verbose=1)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=11, restore_best_weights=True, verbose=1)\n",
    "callback_list = [rlr, es]\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=500, epochs=1000, verbose=1,\n",
    "                    validation_data=(X_val, y_val), callbacks=callback_list\n",
    ")\n",
    "epochs=np.arange(1, len(history.history['loss'])+1)\n",
    "plt.plot(epochs, history.history['loss'], label='training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "epochs=np.arange(2, len(history.history['loss'])+1)\n",
    "plt.plot(epochs, history.history['loss'][1:], label='training loss')\n",
    "plt.plot(epochs, history.history['val_loss'][1:], label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "a = np.array([]).reshape(0, 1)\n",
    "\n",
    "for x in range (1, 42):\n",
    "    length = int((X_test.shape[0] / 41))\n",
    "    test = X_test[length*(x-1):length*(x)]\n",
    "    \n",
    "    y_hat = model.predict(test)\n",
    "    #print(y_hat)\n",
    "    a = np.append(a, y_hat)\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(((a - y_test) / y_test)), axis=0))\n",
    "print('RMSE :', rmse)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
