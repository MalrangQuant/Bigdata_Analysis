{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperResNet(input_shape=(128, 128, 3), classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_tuning (keras_tuner.HyperModel):\n",
    "\n",
    "    def build_model (self, hyperParameter):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                units=hyperParameter.Int('units', min_value=178, max_value=218, step=10),\n",
    "                input_dim=X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation='LeakyReLU'\n",
    "            ))\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                units=hyperParameter.Int('units', min_value=29, max_value=69, step=10),\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                activation='LeakyReLU'\n",
    "            ))\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                units=1,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "        ))\n",
    "        # model.summary()\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                    loss=rmspe)\n",
    "\n",
    "    #     rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_delta=1e-5, verbose=1)\n",
    "    #     es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=11, restore_best_weights=True, verbose=1)\n",
    "    #     callback_list = [rlr, es]\n",
    "    #     history = model.fit(X_train, y_train,\n",
    "    #                     batch_size=500, epochs=1000, verbose=1,\n",
    "    #                     validation_data=(X_val, y_val), callbacks=callback_list\n",
    "    # )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit (self, hyperparameter, model, x, y, validation_data, callbacks=None, **kwangs):\n",
    "\n",
    "        batch_size = hyperparameter.Int(\"batch_size\", 512, 1024, step=128, default=512)\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(\n",
    "            batch_size\n",
    "        )\n",
    "        validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(\n",
    "            batch_size\n",
    "        )\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            hyperparameter.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\", default=1e-3)\n",
    "        )\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        # Function to run the train step.\n",
    "        @tf.function\n",
    "        def run_train_step(images, labels):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(labels, logits)\n",
    "                # Add any regularization losses.\n",
    "                if model.losses:\n",
    "                    loss += tf.math.add_n(model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # The metric to track validation loss.\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        # Function to run the validation step.\n",
    "        @tf.function\n",
    "        def run_val_step(images, labels):\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(labels, logits)\n",
    "            # Update the metric.\n",
    "            epoch_loss_metric.update_state(loss)\n",
    "\n",
    "        # Assign the model to the callbacks.\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "\n",
    "        # Record the best validation loss value\n",
    "        best_epoch_loss = float(\"inf\")\n",
    "\n",
    "        # The custom training loop.\n",
    "        for epoch in range(2):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            # Iterate the training data to run the training step.\n",
    "            for images, labels in train_ds:\n",
    "                run_train_step(images, labels)\n",
    "\n",
    "            # Iterate the validation data to run the validation step.\n",
    "            for images, labels in validation_data:\n",
    "                run_val_step(images, labels)\n",
    "\n",
    "            # Calling the callbacks after epoch.\n",
    "            epoch_loss = float(epoch_loss_metric.result().numpy())\n",
    "            for callback in callbacks:\n",
    "                # The \"my_metric\" is the objective passed to the tuner.\n",
    "                callback.on_epoch_end(epoch, logs={\"my_metric\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "            best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "\n",
    "        # Return the evaluation metric value.\n",
    "        return best_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (hyperParameter):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=hyperParameter.Int('units', min_value=178, max_value=218, step=10),\n",
    "            input_dim=X_train.shape[1],\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            activation='LeakyReLU'\n",
    "        ))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=hyperParameter.Int('units', min_value=29, max_value=69, step=10),\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            activation='LeakyReLU'\n",
    "        ))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "    ))\n",
    "    # model.summary()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                  loss=rmspe)\n",
    "\n",
    "#     rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_delta=1e-5, verbose=1)\n",
    "#     es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=11, restore_best_weights=True, verbose=1)\n",
    "#     callback_list = [rlr, es]\n",
    "#     history = model.fit(X_train, y_train,\n",
    "#                     batch_size=500, epochs=1000, verbose=1,\n",
    "#                     validation_data=(X_val, y_val), callbacks=callback_list\n",
    "# )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kerastuner.Hyperband(hypermodel = build_model,\n",
    "                             objective = 'val_accuracy',\n",
    "                             directory = 'my_dir',\n",
    "                             max_epochs= 100,\n",
    "                             ba\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_delta=1e-5, verbose=1)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=11, restore_best_weights=True, verbose=1)\n",
    "callback_list = [rlr, es]\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=300, validation_data=(X_val, y_val), callbacks=callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
